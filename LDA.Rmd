---
title: "Linear Discriminant Analysis"
output: github_document
---

Linear discriminant analysis (LDA) and the related Fisher’s linear discriminant are methods used in statistics, pattern recognition and machine learning to find a linear combination of features which characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.

LDA assumes that predictors are normally distributed (Gaussian distribution) and that the different classes have class-specific means and equal variance/covariance.

The linear discriminant analysis can be easily computed using the function lda() [MASS package].

```{r setup, include=TRUE,warning=FALSE}
library(datasets)
library(MASS)
library(ggplot2)
library(dplyr)
```

## Linear Discriminant Analysis

Linear Discriminant Analysis (LDA) is a dimensionality reduction technique. As the name implies dimensionality reduction techniques reduce the number of dimensions (i.e. variables) in a dataset while retaining as much information as possible. Linear Discriminant Analysis, or LDA, uses the information from features to create a new axis and projects the data on to the new axis in such a way as to minimizes the variance and maximizes the distance between the means of the  classes.


```{r lda}
summary(iris)
lda.fit<-lda(Species~.,data=iris)
```

Performing Linear Discriminant Analysis on Iris dataset for classification per Species.
LDA determines group means and computes, for each individual, the probability of belonging to the different groups. The individual is then affected to the group with the highest probability score.

The lda() outputs contain the following elements:

Prior probabilities of groups: the proportion of training observations in each group. For example, there are 31% of the training observations in the setosa group
Group means: group center of gravity. Shows the mean of each variable in each group.

Coefficients of linear discriminants: Shows the linear combination of predictor variables that are used to form the LDA decision rule. 

for example, LD1 = 0.82*Sepal.Length + 1.53*Sepal.Width - 2.2*Petal.Length - 2.8*Petal.Width. 

Similarly, LD2 = 0.024*Sepal.Length + 2.164*Sepal.Width - 0.93*Petal.Length +2.83*Petal.Width.

The “proportion of trace” returned by the lda() function is the percentage separation achieved by each discriminant function.  (99.12% and 0.88%).

```{r lda2}
lda.fit
plot(lda.fit)

predictions <- lda.fit %>% predict(iris)
iris <- cbind(iris, predict(lda.fit)$x)
ggplot(iris, aes(LD1, LD2)) +
  geom_point(aes(color = Species))
```

References:
https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2#:~:text=Linear%20Discriminant%20Analysis%20(LDA)%20is,as%20much%20information%20as%20possible.

https://www.displayr.com/linear-discriminant-analysis-in-r-an-introduction/

http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

